import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
import asyncio
from agents.risk_based_news import RiskBasedNewsAgent

class MarketNews:
    def __init__(self):
        self.name = "Market News Agent"
        self.cafef_url = "https://cafef.vn/thi-truong-chung-khoan.chn"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.ai_agent = None  # Will be set by main_agent
        self.risk_news_agent = RiskBasedNewsAgent()
    
    def set_ai_agent(self, ai_agent):
        """Set AI agent for enhanced market news analysis"""
        self.ai_agent = ai_agent
    
    def get_market_news(self, category: str = "general", risk_tolerance: int = 50, time_horizon: str = "Trung háº¡n", investment_amount: int = 10000000):
        try:
            # Get risk-based news first
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                risk_news = loop.run_until_complete(
                    self.risk_news_agent.get_news_by_risk_profile(risk_tolerance, time_horizon, investment_amount)
                )
                loop.close()
                
                if not risk_news.get('error'):
                    base_news = {
                        "category": "Vietnam Market",
                        "news_count": risk_news['total_news'],
                        "news": risk_news['news_data'],
                        "source": risk_news['source_info'],
                        "risk_profile": risk_news['risk_profile'],
                        "news_type": risk_news['news_type'],
                        "recommendation": risk_news['recommendation']
                    }
                else:
                    raise Exception("Risk news failed")
            except:
                # Fallback to traditional news
                cafef_news = self._crawl_cafef_news()
                if cafef_news:
                    base_news = {
                        "category": "Vietnam Market",
                        "news_count": len(cafef_news),
                        "news": cafef_news,
                        "source": "ðŸ“° CafeF.vn (Tin chÃ­nh thá»‘ng)",
                        "risk_profile": "Default",
                        "news_type": "official"
                    }
                else:
                    base_news = self._get_vn_mock_news()
            
            # Enhance with AI analysis if available
            if base_news and self.ai_agent:
                try:
                    ai_enhancement = self._get_ai_market_analysis(base_news, category)
                    base_news.update(ai_enhancement)
                except Exception as e:
                    print(f"âš ï¸ AI market analysis failed: {e}")
                    base_news['ai_enhanced'] = False
                    base_news['ai_error'] = str(e)
            
            return base_news
                
        except Exception as e:
            print(f"âŒ Error crawling CafeF: {e}")
            return self._get_vn_mock_news()
    
    def _crawl_cafef_news(self):
        try:
            response = requests.get(self.cafef_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_items = []
            
            # Find news articles - multiple selectors
            selectors = [
                '.tlitem',
                '.item-news',
                '.news-item',
                '.article-item',
                'article',
                '.box-category-item'
            ]
            
            articles = []
            for selector in selectors:
                found = soup.select(selector)
                if found:
                    articles = found
                    break
            
            # If no specific selectors work, try finding by common patterns
            if not articles:
                articles = soup.find_all(['div', 'article'], class_=re.compile(r'(news|item|article|post)'))
            
            for article in articles[:20]:  # Limit to 20 articles
                try:
                    # Extract title
                    title_elem = article.find(['h1', 'h2', 'h3', 'h4', 'a'], class_=re.compile(r'(title|headline)'))
                    if not title_elem:
                        title_elem = article.find('a')
                    
                    title = title_elem.get_text(strip=True) if title_elem else "KhÃ´ng cÃ³ tiÃªu Ä‘á»"
                    
                    # Extract link
                    link_elem = title_elem if title_elem and title_elem.name == 'a' else article.find('a')
                    link = link_elem.get('href', '') if link_elem else ''
                    if link and not link.startswith('http'):
                        link = 'https://cafef.vn' + link
                    
                    # Extract time
                    time_elem = article.find(['time', 'span'], class_=re.compile(r'(time|date)'))
                    if not time_elem:
                        time_elem = article.find(text=re.compile(r'\d{1,2}[/\-]\d{1,2}'))
                    
                    published = time_elem.get_text(strip=True) if hasattr(time_elem, 'get_text') else str(time_elem) if time_elem else datetime.now().strftime('%d/%m/%Y')
                    
                    # Extract summary
                    summary_elem = article.find(['p', 'div'], class_=re.compile(r'(summary|desc|content)'))
                    summary = summary_elem.get_text(strip=True)[:200] if summary_elem else title[:100] + "..."
                    
                    if title and len(title) > 10:  # Valid title
                        news_items.append({
                            "title": title,
                            "link": link,
                            "published": published,
                            "summary": summary,
                            "publisher": "CafeF",
                            "source_index": "VN Market"
                        })
                        
                except Exception as article_error:
                    continue
            
            print(f"âœ… Crawled {len(news_items)} news from CafeF")
            return news_items
            
        except Exception as e:
            print(f"âŒ CafeF crawling failed: {e}")
            return None
    
    def _get_vn_mock_news(self):
        """Fallback VN market news"""
        import random
        
        mock_news = [
            "VN-Index tÄƒng Ä‘iá»ƒm trong phiÃªn giao dá»‹ch sÃ¡ng nay",
            "Khá»‘i ngoáº¡i mua rÃ²ng 200 tá»· Ä‘á»“ng trÃªn HOSE",
            "NhÃ³m cá»• phiáº¿u ngÃ¢n hÃ ng dáº«n dáº¯t thá»‹ trÆ°á»ng",
            "Thanh khoáº£n thá»‹ trÆ°á»ng cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ",
            "Cá»• phiáº¿u báº¥t Ä‘á»™ng sáº£n cÃ³ dáº¥u hiá»‡u phá»¥c há»“i",
            "Thá»‹ trÆ°á»ng phÃ¡i sinh giao dá»‹ch tÃ­ch cá»±c",
            "HNX-Index tÄƒng nháº¹ theo Ä‘Ã  chung",
            "NhÃ³m cá»• phiáº¿u cÃ´ng nghá»‡ thu hÃºt dÃ²ng tiá»n",
            "Chá»‰ sá»‘ VN30 vÆ°á»£t qua vÃ¹ng khÃ¡ng cá»± quan trá»ng",
            "Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam á»•n Ä‘á»‹nh"
        ]
        
        news_items = []
        for i, title in enumerate(mock_news):
            news_items.append({
                "title": title,
                "link": f"https://cafef.vn/mock-news-{i+1}",
                "published": datetime.now().strftime('%d/%m/%Y %H:%M'),
                "summary": f"PhÃ¢n tÃ­ch chi tiáº¿t vá» {title.lower()}",
                "publisher": "CafeF Mock",
                "source_index": "VN Market"
            })
        
        return {
            "category": "Vietnam Market",
            "news_count": len(news_items),
            "news": news_items,
            "source": "ðŸ“° Mock VN News (Fallback)",
            "risk_profile": "Default",
            "news_type": "official"
        }
    
    def _get_ai_market_analysis(self, base_news: dict, category: str):
        """Get AI-enhanced market analysis"""
        try:
            # Prepare market news context for AI analysis
            news_titles = []
            for news_item in base_news.get('news', []):
                news_titles.append(f"- {news_item.get('title', '')}")
            
            news_context = "\n".join(news_titles[:8])  # Limit to top 8 news
            
            context = f"""
PhÃ¢n tÃ­ch thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam dá»±a trÃªn tin tá»©c má»›i nháº¥t:

THÃ”NG TIN THá»Š TRÆ¯á»œNG:
- Danh má»¥c: {base_news.get('category', 'N/A')}
- Sá»‘ lÆ°á»£ng tin: {base_news.get('news_count', 0)}
- Nguá»“n: {base_news.get('source', 'N/A')}

TIN Tá»¨C THá»Š TRÆ¯á»œNG:
{news_context}

HÃ£y Ä‘Æ°a ra phÃ¢n tÃ­ch chuyÃªn sÃ¢u vá»:
1. Xu hÆ°á»›ng tá»•ng thá»ƒ cá»§a thá»‹ trÆ°á»ng (tÄƒng/giáº£m/sideway)
2. CÃ¡c yáº¿u tá»‘ chÃ­nh tÃ¡c Ä‘á»™ng Ä‘áº¿n thá»‹ trÆ°á»ng
3. NhÃ³m ngÃ nh ná»•i báº­t (tÃ­ch cá»±c/tiÃªu cá»±c)
4. Dá»± bÃ¡o ngáº¯n háº¡n cho thá»‹ trÆ°á»ng
5. Khuyáº¿n nghá»‹ chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°

Tráº£ lá»i ngáº¯n gá»n, táº­p trung vÃ o nhá»¯ng Ä‘iá»ƒm quan trá»ng nháº¥t.
"""
            
            # Get AI analysis
            ai_result = self.ai_agent.generate_with_fallback(context, 'market_analysis', max_tokens=600)
            
            if ai_result['success']:
                return {
                    'ai_market_analysis': ai_result['response'],
                    'ai_model_used': ai_result['model_used'],
                    'ai_enhanced': True,
                    'market_sentiment': self._extract_market_sentiment(ai_result['response']),
                    'market_trend': self._extract_market_trend(ai_result['response'])
                }
            else:
                return {'ai_enhanced': False, 'ai_error': ai_result.get('error', 'AI not available')}
                
        except Exception as e:
            return {'ai_enhanced': False, 'ai_error': str(e)}
    
    def _extract_market_sentiment(self, ai_response: str):
        """Extract market sentiment from AI response"""
        try:
            ai_lower = ai_response.lower()
            
            # Market sentiment indicators
            bullish_indicators = ['tÄƒng', 'tÃ­ch cá»±c', 'bullish', 'kháº£ quan', 'máº¡nh', 'tá»‘t']
            bearish_indicators = ['giáº£m', 'tiÃªu cá»±c', 'bearish', 'yáº¿u', 'lo ngáº¡i', 'rá»§i ro']
            
            bullish_count = sum(1 for indicator in bullish_indicators if indicator in ai_lower)
            bearish_count = sum(1 for indicator in bearish_indicators if indicator in ai_lower)
            
            if bullish_count > bearish_count:
                return 'BULLISH'
            elif bearish_count > bullish_count:
                return 'BEARISH'
            else:
                return 'NEUTRAL'
                
        except Exception:
            return 'NEUTRAL'
    
    def _extract_market_trend(self, ai_response: str):
        """Extract market trend from AI response"""
        try:
            ai_lower = ai_response.lower()
            
            if any(phrase in ai_lower for phrase in ['xu hÆ°á»›ng tÄƒng', 'uptrend', 'tÄƒng trÆ°á»Ÿng']):
                return 'UPTREND'
            elif any(phrase in ai_lower for phrase in ['xu hÆ°á»›ng giáº£m', 'downtrend', 'suy giáº£m']):
                return 'DOWNTREND'
            elif any(phrase in ai_lower for phrase in ['sideway', 'Ä‘i ngang', 'consolidation']):
                return 'SIDEWAYS'
            else:
                return 'MIXED'
                
        except Exception:
            return 'MIXED'