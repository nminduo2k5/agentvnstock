import google.generativeai as genai
import openai
import os
import logging
from dotenv import load_dotenv
from typing import Dict, Any, Optional, List
import asyncio
import json

load_dotenv()
logger = logging.getLogger(__name__)

class UnifiedAIAgent:
    def __init__(self, gemini_api_key: str = None, openai_api_key: str = None):
        """
        Initialize Unified AI Agent with multiple AI models
        """
        self.available_models = {}
        self.model_capabilities = {
            'gemini': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice'],
                'speed': 'fast',
                'cost': 'low'
            },
            'chatgpt': {
                'strengths': ['prediction', 'technical_analysis', 'news_analysis', 'risk_assessment'],
                'speed': 'medium',
                'cost': 'medium'
            }
        }
        
        # Initialize Gemini
        if not gemini_api_key:
            gemini_api_key = os.getenv('GOOGLE_API_KEY')
        
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                model_name = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
                self.available_models['gemini'] = genai.GenerativeModel(model_name)
                self.gemini_api_key = gemini_api_key
                logger.info("‚úÖ Gemini AI initialized successfully")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize Gemini: {str(e)}")
        
        # Initialize OpenAI
        if not openai_api_key:
            openai_api_key = os.getenv('OPENAI_API_KEY')
        
        if openai_api_key:
            try:
                self.openai_client = openai.OpenAI(api_key=openai_api_key)
                self.available_models['chatgpt'] = self.openai_client
                self.openai_api_key = openai_api_key
                logger.info("‚úÖ ChatGPT AI initialized successfully")
            except Exception as e:
                logger.error(f"‚ùå Failed to initialize ChatGPT: {str(e)}")
        
        if not self.available_models:
            raise ValueError("At least one AI model must be configured. Please provide API keys.")
    
    def test_connection(self):
        """Test API connections for all available models"""
        results = {}
        
        if 'gemini' in self.available_models:
            try:
                response = self.available_models['gemini'].generate_content("Hello")
                results['gemini'] = True
                logger.info("‚úÖ Gemini connection test passed")
            except Exception as e:
                results['gemini'] = False
                logger.error(f"‚ùå Gemini connection test failed: {str(e)}")
        
        if 'chatgpt' in self.available_models:
            try:
                response = self.available_models['chatgpt'].chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": "Hello"}],
                    max_tokens=10
                )
                results['chatgpt'] = True
                logger.info("‚úÖ ChatGPT connection test passed")
            except Exception as e:
                results['chatgpt'] = False
                logger.error(f"‚ùå ChatGPT connection test failed: {str(e)}")
        
        if not any(results.values()):
            raise ValueError("All API connection tests failed")
        
        return results
    
    def select_best_model(self, task_type: str) -> str:
        """
        Automatically select the best AI model for a specific task type
        """
        task_model_mapping = {
            'financial_advice': 'gemini',  # Gemini better for Vietnamese financial advice
            'price_prediction': 'chatgpt',  # ChatGPT better for technical analysis
            'risk_assessment': 'chatgpt',   # ChatGPT better for risk calculations
            'news_analysis': 'chatgpt',     # ChatGPT better for news sentiment
            'market_analysis': 'gemini',    # Gemini better for market reasoning
            'investment_analysis': 'chatgpt', # ChatGPT better for investment metrics
            'general_query': 'gemini'       # Gemini better for general Vietnamese queries
        }
        
        preferred_model = task_model_mapping.get(task_type, 'gemini')
        
        # Check if preferred model is available, otherwise use any available
        if preferred_model in self.available_models:
            return preferred_model
        elif self.available_models:
            return list(self.available_models.keys())[0]
        else:
            raise ValueError("No AI models available")
    
    def generate_with_model(self, prompt: str, model_name: str, max_tokens: int = 1000) -> str:
        """
        Generate response using specific AI model
        """
        try:
            if model_name == 'gemini' and 'gemini' in self.available_models:
                response = self.available_models['gemini'].generate_content(prompt)
                return response.text
            
            elif model_name == 'chatgpt' and 'chatgpt' in self.available_models:
                response = self.available_models['chatgpt'].chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                return response.choices[0].message.content
            
            else:
                raise ValueError(f"Model {model_name} not available")
                
        except Exception as e:
            logger.error(f"Error generating with {model_name}: {str(e)}")
            raise
    
    def generate_with_fallback(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """
        Generate response with automatic fallback to other models if primary fails
        """
        primary_model = self.select_best_model(task_type)
        
        try:
            response = self.generate_with_model(prompt, primary_model, max_tokens)
            return {
                'response': response,
                'model_used': primary_model,
                'success': True
            }
        except Exception as e:
            logger.warning(f"Primary model {primary_model} failed: {str(e)}")
            
            # Try other available models
            for model_name in self.available_models.keys():
                if model_name != primary_model:
                    try:
                        response = self.generate_with_model(prompt, model_name, max_tokens)
                        return {
                            'response': response,
                            'model_used': model_name,
                            'success': True,
                            'fallback': True
                        }
                    except Exception as fallback_error:
                        logger.warning(f"Fallback model {model_name} also failed: {str(fallback_error)}")
                        continue
            
            # If all models fail
            return {
                'response': f"L·ªói: T·∫•t c·∫£ AI models ƒë·ªÅu kh√¥ng kh·∫£ d·ª•ng. Task: {task_type}",
                'model_used': None,
                'success': False,
                'error': str(e)
            }
    
    def generate_expert_advice(self, query: str, symbol: str = None, data: dict = None):
        """Generate expert financial advice using best available AI model with fallback"""
        
        # Detect query type for better handling
        query_type = self.detect_query_type(query)
        
        # Handle general questions without stock context
        if not symbol and query_type == "general":
            return self.generate_general_response(query)
        
        # Build comprehensive context for stock analysis
        context = f"""
B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh h√†ng ƒë·∫ßu v·ªõi 20 nƒÉm kinh nghi·ªám ƒë·∫ßu t∆∞ ch·ª©ng kho√°n t·∫°i Vi·ªát Nam v√† qu·ªëc t·∫ø.
H√£y ph√¢n t√≠ch s√¢u s·∫Øc v√† ƒë∆∞a ra l·ªùi khuy√™n chuy√™n nghi·ªáp nh·∫•t.

C√ÇU H·ªéI: {query}
M√É C·ªî PHI·∫æU: {symbol if symbol else 'Kh√¥ng c√≥'}
"""
        
        if data:
            context += f"\nD·ªÆ LI·ªÜU PH√ÇN T√çCH CHI TI·∫æT:\n{self._format_data_for_ai(data)}\n"
        
        context += f"""
Y√äU C·∫¶U PH√ÇN T√çCH CHUY√äN S√ÇU:
1. üìä PH√ÇN T√çCH K·ª∏ THU·∫¨T: ƒê√°nh gi√° xu h∆∞·ªõng, momentum, support/resistance
2. üí∞ PH√ÇN T√çCH C∆† B·∫¢N: P/E, P/B, tƒÉng tr∆∞·ªüng, t√†i ch√≠nh
3. üìà PH√ÇN T√çCH TH·ªä TR∆Ø·ªúNG: V·ªã th·∫ø ng√†nh, tri·ªÉn v·ªçng, r·ªßi ro vƒ© m√¥
4. ‚öñÔ∏è ƒê√ÅNH GI√Å R·ª¶I RO: M·ª©c ƒë·ªô r·ªßi ro, kh·∫£ nƒÉng ch·ªãu ƒë·ª±ng
5. üéØ CHI·∫æN L∆Ø·ª¢C ƒê·∫¶U T∆Ø: Ng·∫Øn h·∫°n vs d√†i h·∫°n, timing
6. üí° KHUY·∫æN NGH·ªä C·ª§ TH·ªÇ: Mua/B√°n/Gi·ªØ v·ªõi l√Ω do r√µ r√†ng

H√ÉY TR·∫¢ L·ªúI THEO FORMAT SAU:

PH√ÇN T√çCH CHUY√äN S√ÇU:
[Ph√¢n t√≠ch to√†n di·ªán t·ª´ nhi·ªÅu g√≥c ƒë·ªô, s·ª≠ d·ª•ng d·ªØ li·ªáu c·ª• th·ªÉ]

K·∫æT LU·∫¨N & KHUY·∫æN NGH·ªä:
[K·∫øt lu·∫≠n r√µ r√†ng: N√äN/KH√îNG N√äN ƒë·∫ßu t∆∞ v·ªõi l√Ω do thuy·∫øt ph·ª•c]

H√ÄNH ƒê·ªòNG C·ª§ TH·ªÇ:
- [Danh s√°ch 4-5 h√†nh ƒë·ªông c·ª• th·ªÉ]

C·∫¢NH B√ÅO R·ª¶I RO:
[Nh·ªØng r·ªßi ro quan tr·ªçng c·∫ßn l∆∞u √Ω]

L∆∞u √Ω: Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø, kh√¥ng ƒë∆∞a l·ªùi khuy√™n tuy·ªát ƒë·ªëi.
"""
        
        # Use the new unified AI system with fallback
        try:
            result = self.generate_with_fallback(context, 'financial_advice', max_tokens=2048)
            
            if result['success']:
                parsed_response = self._parse_response(result['response'])
                
                # Add model info to response
                if result.get('fallback'):
                    parsed_response['expert_advice'] += f"\n\nü§ñ **AI Model:** {result['model_used']} (fallback)"
                else:
                    parsed_response['expert_advice'] += f"\n\nü§ñ **AI Model:** {result['model_used']}"
                
                return parsed_response
            else:
                return {
                    "expert_advice": f"‚ùå **L·ªñI AI SYSTEM:**\n{result.get('response', 'Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi AI models')}\n\n‚ö†Ô∏è **G·ª¢I √ù:**\n- Ki·ªÉm tra API keys\n- Th·ª≠ l·∫°i sau v√†i ph√∫t\n- Li√™n h·ªá h·ªó tr·ª£ n·∫øu v·∫•n ƒë·ªÅ ti·∫øp t·ª•c",
                    "recommendations": [
                        "Ki·ªÉm tra API keys (Gemini/OpenAI)",
                        "Th·ª≠ l·∫°i sau v√†i ph√∫t", 
                        "Li√™n h·ªá h·ªó tr·ª£ k·ªπ thu·∫≠t",
                        "S·ª≠ d·ª•ng ch·∫ø ƒë·ªô offline"
                    ]
                }
                
        except Exception as e:
            logger.error(f"Critical error in generate_expert_advice: {str(e)}")
            return {
                "expert_advice": f"‚ùå **L·ªñI NGHI√äM TR·ªåNG:**\n{str(e)}\n\n‚ö†Ô∏è H·ªá th·ªëng AI t·∫°m th·ªùi kh√¥ng kh·∫£ d·ª•ng.",
                "recommendations": [
                    "Th·ª≠ l·∫°i sau 5-10 ph√∫t",
                    "Ki·ªÉm tra k·∫øt n·ªëi internet",
                    "Li√™n h·ªá h·ªó tr·ª£ k·ªπ thu·∫≠t"
                ]
            }
    
    def _parse_response(self, response_text: str):
        """Parse enhanced Gemini response"""
        try:
            # Parse different sections
            sections = {
                'analysis': '',
                'conclusion': '',
                'actions': [],
                'risks': ''
            }
            
            # Split by sections
            if "PH√ÇN T√çCH CHUY√äN S√ÇU:" in response_text:
                parts = response_text.split("PH√ÇN T√çCH CHUY√äN S√ÇU:")
                if len(parts) > 1:
                    remaining = parts[1]
                    
                    # Extract analysis
                    if "K·∫æT LU·∫¨N & KHUY·∫æN NGH·ªä:" in remaining:
                        analysis_part = remaining.split("K·∫æT LU·∫¨N & KHUY·∫æN NGH·ªä:")[0].strip()
                        sections['analysis'] = analysis_part
                        remaining = remaining.split("K·∫æT LU·∫¨N & KHUY·∫æN NGH·ªä:")[1]
                    
                    # Extract conclusion
                    if "H√ÄNH ƒê·ªòNG C·ª§ TH·ªÇ:" in remaining:
                        conclusion_part = remaining.split("H√ÄNH ƒê·ªòNG C·ª§ TH·ªÇ:")[0].strip()
                        sections['conclusion'] = conclusion_part
                        remaining = remaining.split("H√ÄNH ƒê·ªòNG C·ª§ TH·ªÇ:")[1]
                    
                    # Extract actions
                    if "C·∫¢NH B√ÅO R·ª¶I RO:" in remaining:
                        actions_part = remaining.split("C·∫¢NH B√ÅO R·ª¶I RO:")[0].strip()
                        sections['risks'] = remaining.split("C·∫¢NH B√ÅO R·ª¶I RO:")[1].strip()
                    else:
                        actions_part = remaining.strip()
                    
                    # Parse actions list
                    for line in actions_part.split('\n'):
                        line = line.strip()
                        if line and (line.startswith('-') or line.startswith('‚Ä¢') or line.startswith('*')):
                            sections['actions'].append(line[1:].strip())
                        elif line and len(line) > 15 and not line.startswith('C·∫¢NH B√ÅO'):
                            sections['actions'].append(line)
            
            # Format comprehensive response
            expert_advice = f"""
üìä **PH√ÇN T√çCH CHUY√äN S√ÇU:**
{sections['analysis']}

üéØ **K·∫æT LU·∫¨N & KHUY·∫æN NGH·ªä:**
{sections['conclusion']}

‚ö†Ô∏è **C·∫¢NH B√ÅO R·ª¶I RO:**
{sections['risks'] if sections['risks'] else 'Lu√¥n c√≥ r·ªßi ro trong ƒë·∫ßu t∆∞. Ch·ªâ ƒë·∫ßu t∆∞ s·ªë ti·ªÅn c√≥ th·ªÉ ch·∫•p nh·∫≠n m·∫•t.'}
""".strip()
            
            return {
                "expert_advice": expert_advice,
                "recommendations": sections['actions'][:5] if sections['actions'] else [
                    "Nghi√™n c·ª©u k·ªπ b√°o c√°o t√†i ch√≠nh",
                    "Theo d√µi tin t·ª©c ng√†nh", 
                    "ƒê·∫∑t l·ªánh stop-loss",
                    "ƒêa d·∫°ng h√≥a danh m·ª•c",
                    "Ch·ªâ ƒë·∫ßu t∆∞ ti·ªÅn nh√†n r·ªói"
                ]
            }
                
        except Exception as e:
            # Fallback parsing
            return {
                "expert_advice": f"üìà **PH√ÇN T√çCH:**\n{response_text}\n\n‚ö†Ô∏è **L∆ØU √ù:** ƒê√¢y ch·ªâ l√† tham kh·∫£o, kh√¥ng ph·∫£i l·ªùi khuy√™n ƒë·∫ßu t∆∞ tuy·ªát ƒë·ªëi.",
                "recommendations": [
                    "Nghi√™n c·ª©u th√™m t·ª´ nhi·ªÅu ngu·ªìn",
                    "Tham kh·∫£o chuy√™n gia t√†i ch√≠nh",
                    "ƒê√°nh gi√° kh·∫£ nƒÉng t√†i ch√≠nh c√° nh√¢n",
                    "Ch·ªâ ƒë·∫ßu t∆∞ s·ªë ti·ªÅn c√≥ th·ªÉ ch·∫•p nh·∫≠n m·∫•t"
                ]
            }
    
    def _format_data_for_ai(self, data: dict) -> str:
        """Format data for AI analysis"""
        if not data:
            return "Kh√¥ng c√≥ d·ªØ li·ªáu c·ª• th·ªÉ"
        
        formatted = []
        
        # VN Stock Data
        if 'vn_stock_data' in data and data['vn_stock_data']:
            vn_data = data['vn_stock_data']
            formatted.append(f"üìà TH√îNG TIN C·ªî PHI·∫æU:")
            formatted.append(f"- Gi√° hi·ªán t·∫°i: {vn_data.price:,} VND")
            formatted.append(f"- Thay ƒë·ªïi: {vn_data.change:+,} VND ({vn_data.change_percent:+.2f}%)")
            formatted.append(f"- Kh·ªëi l∆∞·ª£ng: {vn_data.volume:,}")
            formatted.append(f"- V·ªën h√≥a: {vn_data.market_cap:,} t·ª∑ VND")
            formatted.append(f"- P/E: {vn_data.pe_ratio}, P/B: {vn_data.pb_ratio}")
            formatted.append(f"- Ng√†nh: {vn_data.sector}, S√†n: {vn_data.exchange}")
        
        # Price Prediction
        if 'price_prediction' in data and data['price_prediction']:
            pred = data['price_prediction']
            formatted.append(f"\nüîÆ D·ª∞ ƒêO√ÅN GI√Å:")
            formatted.append(f"- Xu h∆∞·ªõng: {pred.get('trend', 'N/A')}")
            formatted.append(f"- Gi√° d·ª± ƒëo√°n: {pred.get('predicted_price', 'N/A')}")
            formatted.append(f"- ƒê·ªô tin c·∫≠y: {pred.get('confidence', 'N/A')}")
        
        # Risk Assessment
        if 'risk_assessment' in data and data['risk_assessment']:
            risk = data['risk_assessment']
            formatted.append(f"\n‚ö†Ô∏è ƒê√ÅNH GI√Å R·ª¶I RO:")
            formatted.append(f"- M·ª©c r·ªßi ro: {risk.get('risk_level', 'N/A')}")
            formatted.append(f"- ƒê·ªô bi·∫øn ƒë·ªông: {risk.get('volatility', 'N/A')}%")
            formatted.append(f"- Beta: {risk.get('beta', 'N/A')}")
            formatted.append(f"- Max Drawdown: {risk.get('max_drawdown', 'N/A')}%")
        
        # Investment Analysis
        if 'investment_analysis' in data and data['investment_analysis']:
            inv = data['investment_analysis']
            formatted.append(f"\nüíº PH√ÇN T√çCH ƒê·∫¶U T∆Ø:")
            formatted.append(f"- Khuy·∫øn ngh·ªã: {inv.get('recommendation', 'N/A')}")
            formatted.append(f"- L√Ω do: {inv.get('reason', 'N/A')}")
            formatted.append(f"- C·ªï t·ª©c: {inv.get('dividend_yield', 'N/A')}%")
            formatted.append(f"- Gi√° m·ª•c ti√™u: {inv.get('target_price', 'N/A')}")
        
        return "\n".join(formatted) if formatted else "D·ªØ li·ªáu kh√¥ng ƒë·∫ßy ƒë·ªß ƒë·ªÉ ph√¢n t√≠ch"
    
    def generate_general_response(self, query: str) -> dict:
        """Generate response for general questions using best available AI model"""
        try:
            # Enhanced context for general financial questions
            context = f"""
B·∫°n l√† m·ªôt chuy√™n gia t√†i ch√≠nh v√† ƒë·∫ßu t∆∞ h√†ng ƒë·∫ßu t·∫°i Vi·ªát Nam v·ªõi 20+ nƒÉm kinh nghi·ªám.
B·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi m·ªçi c√¢u h·ªèi v·ªÅ:
- Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam v√† qu·ªëc t·∫ø
- Ph√¢n t√≠ch k·ªπ thu·∫≠t v√† c∆° b·∫£n
- Chi·∫øn l∆∞·ª£c ƒë·∫ßu t∆∞ v√† qu·∫£n l√Ω r·ªßi ro
- Kinh t·∫ø vƒ© m√¥ v√† vi m√¥
- C√°c s·∫£n ph·∫©m t√†i ch√≠nh (c·ªï phi·∫øu, tr√°i phi·∫øu, qu·ªπ, forex)
- L·∫≠p k·∫ø ho·∫°ch t√†i ch√≠nh c√° nh√¢n
- Thu·∫ø v√† ph√°p l√Ω ƒë·∫ßu t∆∞
- T√¢m l√Ω h·ªçc ƒë·∫ßu t∆∞
- Fintech v√† c√¥ng ngh·ªá t√†i ch√≠nh

C√ÇU H·ªéI: {query}

H√ÉY TR·∫¢ L·ªúI:
1. üìö KI·∫æN TH·ª®C C∆† B·∫¢N: Gi·∫£i th√≠ch kh√°i ni·ªám/v·∫•n ƒë·ªÅ
2. üéØ PH√ÇN T√çCH TH·ª∞C T·∫æ: √Åp d·ª•ng v√†o th·ªã tr∆∞·ªùng VN
3. üí° KHUY·∫æN NGH·ªä: L·ªùi khuy√™n c·ª• th·ªÉ v√† th·ª±c t·∫ø
4. ‚ö†Ô∏è L∆ØU √ù: R·ªßi ro v√† ƒëi·ªÅu c·∫ßn ch√∫ √Ω

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp nh∆∞ng d·ªÖ hi·ªÉu.
"""
            
            # Use unified AI system with fallback
            result = self.generate_with_fallback(context, 'general_query', max_tokens=2048)
            
            if result['success']:
                return {
                    "expert_advice": f"üìà **PH√ÇN T√çCH CHUY√äN GIA:**\n{result['response']}\n\nü§ñ **AI Model:** {result['model_used']}\n\n‚ö†Ô∏è **L∆ØU √ù:** ƒê√¢y l√† th√¥ng tin tham kh·∫£o, kh√¥ng ph·∫£i l·ªùi khuy√™n ƒë·∫ßu t∆∞ tuy·ªát ƒë·ªëi.",
                    "recommendations": [
                        "Nghi√™n c·ª©u th√™m t·ª´ nhi·ªÅu ngu·ªìn",
                        "Tham kh·∫£o chuy√™n gia t√†i ch√≠nh", 
                        "ƒê√°nh gi√° kh·∫£ nƒÉng t√†i ch√≠nh c√° nh√¢n",
                        "Ch·ªâ ƒë·∫ßu t∆∞ s·ªë ti·ªÅn c√≥ th·ªÉ ch·∫•p nh·∫≠n m·∫•t"
                    ]
                }
            else:
                return self._get_fallback_response(query)
                
        except Exception as e:
            logger.error(f"Error in generate_general_response: {str(e)}")
            return self._get_fallback_response(query)
    
    def _get_fallback_response(self, query: str) -> dict:
        """Fallback response when Gemini fails"""
        return {
            "expert_advice": f"üìà **V·ªÄ C√ÇU H·ªéI: {query}**\n\nXin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y l√∫c n√†y. Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c ƒë·∫∑t c√¢u h·ªèi kh√°c.\n\n‚ö†Ô∏è **G·ª¢I √ù:**\n- Ki·ªÉm tra k·∫øt n·ªëi internet\n- Th·ª≠ ƒë·∫∑t c√¢u h·ªèi ng·∫Øn g·ªçn h∆°n\n- Li√™n h·ªá h·ªó tr·ª£ n·∫øu v·∫•n ƒë·ªÅ ti·∫øp t·ª•c",
            "recommendations": [
                "Th·ª≠ ƒë·∫∑t c√¢u h·ªèi kh√°c",
                "Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng",
                "Li√™n h·ªá h·ªó tr·ª£ k·ªπ thu·∫≠t"
            ]
        }
    
    def detect_query_type(self, query: str) -> str:
        """Detect if query is stock-specific or general"""
        query_lower = query.lower()
        
        # Stock symbols patterns
        stock_patterns = ['vcb', 'bid', 'ctg', 'vic', 'vhm', 'hpg', 'fpt', 'msn', 'mwg', 'gas', 'plx']
        
        # Check for stock symbols
        for pattern in stock_patterns:
            if pattern in query_lower:
                return "stock_specific"
        
        # Check for stock-related keywords
        stock_keywords = ['c·ªï phi·∫øu', 'm√£', 'ticker', 'stock', 'share']
        if any(keyword in query_lower for keyword in stock_keywords):
            return "stock_specific"
        
        return "general"

# Backward compatibility alias
GeminiAgent = UnifiedAIAgent