import google.generativeai as genai
import os
import logging
from dotenv import load_dotenv
from typing import Dict, Any, Optional, List
import asyncio
import json
import time
from datetime import datetime

load_dotenv()
logger = logging.getLogger(__name__)

class UnifiedAIAgent:
    def __init__(self, gemini_api_key: str = None):
        """
        Initialize AI Agent with Gemini AI
        """
        self.available_models = {}
        self.model_capabilities = {
            'gemini': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'news_analysis', 'risk_assessment'],
                'speed': 'fast',
                'cost': 'low'
            }
        }
        
        # Initialize Gemini
        if not gemini_api_key:
            gemini_api_key = os.getenv('GOOGLE_API_KEY')
        
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                model_name = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
                self.available_models['gemini'] = genai.GenerativeModel(model_name)
                self.gemini_api_key = gemini_api_key
                logger.info("âœ… Gemini AI initialized successfully")
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Gemini: {str(e)}")
        
        if not self.available_models:
            raise ValueError("Gemini AI must be configured. Please provide GOOGLE_API_KEY.")
    
    def test_connection(self):
        """Test Gemini API connection"""
        results = {}
        
        if 'gemini' in self.available_models:
            try:
                response = self.available_models['gemini'].generate_content("Hello")
                results['gemini'] = True
                logger.info("âœ… Gemini connection test passed")
            except Exception as e:
                results['gemini'] = False
                logger.error(f"âŒ Gemini connection test failed: {str(e)}")
        
        if not any(results.values()):
            raise ValueError("Gemini API connection test failed")
        
        return results
    
    def select_best_model(self, task_type: str) -> str:
        """
        Select the best available model for a specific task type
        """
        # Since we only have Gemini, always return it if available
        if 'gemini' in self.available_models:
            return 'gemini'
        
        raise ValueError("Gemini AI model not available")
    
    def generate_with_model(self, prompt: str, model_name: str, max_tokens: int = 1000) -> str:
        """
        Generate response using Gemini AI model
        """
        try:
            if model_name == 'gemini' and 'gemini' in self.available_models:
                response = self.available_models['gemini'].generate_content(prompt)
                return response.text
            else:
                raise ValueError(f"Model {model_name} not available. Only Gemini is supported.")
                
        except Exception as e:
            logger.error(f"Error generating with {model_name}: {str(e)}")
            raise
    
    def generate_with_fallback(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """
        Generate response with automatic fallback to other models if primary fails
        """
        primary_model = self.select_best_model(task_type)
        
        try:
            response = self.generate_with_model(prompt, 'gemini', max_tokens)
            return {
                'response': response,
                'model_used': 'gemini',
                'success': True
            }
        except Exception as e:
            logger.error(f"Gemini model failed: {str(e)}")
            return {
                'response': f'Gemini AI failed: {str(e)}',
                'model_used': None,
                'success': False,
                'error': str(e)
            }
    
    def generate_expert_advice(self, query: str, symbol: str = None, data: dict = None):
        """Generate expert financial advice using best available AI model with fallback"""
        
        # Detect query type for better handling
        query_type = self.detect_query_type(query)
        
        # Handle general questions without stock context
        if not symbol and query_type == "general":
            return self.generate_general_response(query)
        
        # Build comprehensive context for stock analysis
        context = f"""
Báº¡n lÃ  má»™t chuyÃªn gia tÃ i chÃ­nh hÃ ng Ä‘áº§u vá»›i 20 nÄƒm kinh nghiá»‡m Ä‘áº§u tÆ° chá»©ng khoÃ¡n táº¡i Viá»‡t Nam vÃ  quá»‘c táº¿.
HÃ£y phÃ¢n tÃ­ch sÃ¢u sáº¯c vÃ  Ä‘Æ°a ra lá»i khuyÃªn chuyÃªn nghiá»‡p nháº¥t.

CÃ‚U Há»ŽI: {query}
MÃƒ Cá»” PHIáº¾U: {symbol if symbol else 'KhÃ´ng cÃ³'}
"""
        
        if data:
            context += f"\nDá»® LIá»†U PHÃ‚N TÃCH CHI TIáº¾T:\n{self._format_data_for_ai(data)}\n"
        
        context += f"""
YÃŠU Cáº¦U PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
1. ðŸ“Š PHÃ‚N TÃCH Ká»¸ THUáº¬T: ÄÃ¡nh giÃ¡ xu hÆ°á»›ng, momentum, support/resistance
2. ðŸ’° PHÃ‚N TÃCH CÆ  Báº¢N: P/E, P/B, tÄƒng trÆ°á»Ÿng, tÃ i chÃ­nh
3. ðŸ“ˆ PHÃ‚N TÃCH THá»Š TRÆ¯á»œNG: Vá»‹ tháº¿ ngÃ nh, triá»ƒn vá»ng, rá»§i ro vÄ© mÃ´
4. âš–ï¸ ÄÃNH GIÃ Rá»¦I RO: Má»©c Ä‘á»™ rá»§i ro, kháº£ nÄƒng chá»‹u Ä‘á»±ng
5. ðŸŽ¯ CHIáº¾N LÆ¯á»¢C Äáº¦U TÆ¯: Ngáº¯n háº¡n vs dÃ i háº¡n, timing
6. ðŸ’¡ KHUYáº¾N NGHá»Š Cá»¤ THá»‚: Mua/BÃ¡n/Giá»¯ vá»›i lÃ½ do rÃµ rÃ ng

HÃƒY TRáº¢ Lá»œI THEO FORMAT SAU:

PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
[PhÃ¢n tÃ­ch toÃ n diá»‡n tá»« nhiá»u gÃ³c Ä‘á»™, sá»­ dá»¥ng dá»¯ liá»‡u cá»¥ thá»ƒ]

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
[Káº¿t luáº­n rÃµ rÃ ng: NÃŠN/KHÃ”NG NÃŠN Ä‘áº§u tÆ° vá»›i lÃ½ do thuyáº¿t phá»¥c]

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- [Danh sÃ¡ch 4-5 hÃ nh Ä‘á»™ng cá»¥ thá»ƒ]

Cáº¢NH BÃO Rá»¦I RO:
[Nhá»¯ng rá»§i ro quan trá»ng cáº§n lÆ°u Ã½]

LÆ°u Ã½: Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿, khÃ´ng Ä‘Æ°a lá»i khuyÃªn tuyá»‡t Ä‘á»‘i.
"""
        
        # Use the new unified AI system with fallback
        try:
            result = self.generate_with_fallback(context, 'financial_advice', max_tokens=2048)
            
            if result['success']:
                parsed_response = self._parse_response(result['response'])
                
                # Add model info to response
                if result.get('fallback'):
                    parsed_response['expert_advice'] += f"\n\nðŸ¤– **AI Model:** {result['model_used']} (fallback)"
                else:
                    parsed_response['expert_advice'] += f"\n\nðŸ¤– **AI Model:** {result['model_used']}"
                
                return parsed_response
            else:
                return {
                    "expert_advice": f"âŒ **Lá»–I AI SYSTEM:**\n{result.get('response', 'KhÃ´ng thá»ƒ káº¿t ná»‘i vá»›i AI models')}\n\nâš ï¸ **Gá»¢I Ã:**\n- Kiá»ƒm tra API keys\n- Thá»­ láº¡i sau vÃ i phÃºt\n- LiÃªn há»‡ há»— trá»£ náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c",
                    "recommendations": [
                        "Kiá»ƒm tra Gemini API key",
                        "Thá»­ láº¡i sau vÃ i phÃºt", 
                        "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t",
                        "Sá»­ dá»¥ng cháº¿ Ä‘á»™ offline"
                    ]
                }
                
        except Exception as e:
            logger.error(f"Critical error in generate_expert_advice: {str(e)}")
            return {
                "expert_advice": f"âŒ **Lá»–I NGHIÃŠM TRá»ŒNG:**\n{str(e)}\n\nâš ï¸ Há»‡ thá»‘ng AI táº¡m thá»i khÃ´ng kháº£ dá»¥ng.",
                "recommendations": [
                    "Thá»­ láº¡i sau 5-10 phÃºt",
                    "Kiá»ƒm tra káº¿t ná»‘i internet",
                    "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t"
                ]
            }
    
    def _parse_response(self, response_text: str):
        """Parse enhanced Gemini response"""
        try:
            # Parse different sections
            sections = {
                'analysis': '',
                'conclusion': '',
                'actions': [],
                'risks': ''
            }
            
            # Split by sections
            if "PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:" in response_text:
                parts = response_text.split("PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:")
                if len(parts) > 1:
                    remaining = parts[1]
                    
                    # Extract analysis
                    if "Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:" in remaining:
                        analysis_part = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[0].strip()
                        sections['analysis'] = analysis_part
                        remaining = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[1]
                    
                    # Extract conclusion
                    if "HÃ€NH Äá»˜NG Cá»¤ THá»‚:" in remaining:
                        conclusion_part = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[0].strip()
                        sections['conclusion'] = conclusion_part
                        remaining = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[1]
                    
                    # Extract actions
                    if "Cáº¢NH BÃO Rá»¦I RO:" in remaining:
                        actions_part = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[0].strip()
                        sections['risks'] = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[1].strip()
                    else:
                        actions_part = remaining.strip()
                    
                    # Parse actions list
                    for line in actions_part.split('\n'):
                        line = line.strip()
                        if line and (line.startswith('-') or line.startswith('â€¢') or line.startswith('*')):
                            sections['actions'].append(line[1:].strip())
                        elif line and len(line) > 15 and not line.startswith('Cáº¢NH BÃO'):
                            sections['actions'].append(line)
            
            # Format comprehensive response
            expert_advice = f"""
ðŸ“Š **PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:**
{sections['analysis']}

ðŸŽ¯ **Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:**
{sections['conclusion']}

âš ï¸ **Cáº¢NH BÃO Rá»¦I RO:**
{sections['risks'] if sections['risks'] else 'LuÃ´n cÃ³ rá»§i ro trong Ä‘áº§u tÆ°. Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t.'}
""".strip()
            
            return {
                "expert_advice": expert_advice,
                "recommendations": sections['actions'][:5] if sections['actions'] else [
                    "NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh",
                    "Theo dÃµi tin tá»©c ngÃ nh", 
                    "Äáº·t lá»‡nh stop-loss",
                    "Äa dáº¡ng hÃ³a danh má»¥c",
                    "Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i"
                ]
            }
                
        except Exception as e:
            # Fallback parsing
            return {
                "expert_advice": f"ðŸ“ˆ **PHÃ‚N TÃCH:**\n{response_text}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y chá»‰ lÃ  tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                "recommendations": [
                    "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                    "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh",
                    "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                    "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                ]
            }
    
    def _format_data_for_ai(self, data: dict) -> str:
        """Format data for AI analysis"""
        if not data:
            return "KhÃ´ng cÃ³ dá»¯ liá»‡u cá»¥ thá»ƒ"
        
        formatted = []
        
        # VN Stock Data
        if 'vn_stock_data' in data and data['vn_stock_data']:
            vn_data = data['vn_stock_data']
            formatted.append(f"ðŸ“ˆ THÃ”NG TIN Cá»” PHIáº¾U:")
            formatted.append(f"- GiÃ¡ hiá»‡n táº¡i: {vn_data.price:,} VND")
            formatted.append(f"- Thay Ä‘á»•i: {vn_data.change:+,} VND ({vn_data.change_percent:+.2f}%)")
            formatted.append(f"- Khá»‘i lÆ°á»£ng: {vn_data.volume:,}")
            formatted.append(f"- Vá»‘n hÃ³a: {vn_data.market_cap:,} tá»· VND")
            formatted.append(f"- P/E: {vn_data.pe_ratio}, P/B: {vn_data.pb_ratio}")
            formatted.append(f"- NgÃ nh: {vn_data.sector}, SÃ n: {vn_data.exchange}")
        
        # Price Prediction
        if 'price_prediction' in data and data['price_prediction']:
            pred = data['price_prediction']
            formatted.append(f"\nðŸ”® Dá»° ÄOÃN GIÃ:")
            formatted.append(f"- Xu hÆ°á»›ng: {pred.get('trend', 'N/A')}")
            formatted.append(f"- GiÃ¡ dá»± Ä‘oÃ¡n: {pred.get('predicted_price', 'N/A')}")
            formatted.append(f"- Äá»™ tin cáº­y: {pred.get('confidence', 'N/A')}")
        
        # Risk Assessment
        if 'risk_assessment' in data and data['risk_assessment']:
            risk = data['risk_assessment']
            formatted.append(f"\nâš ï¸ ÄÃNH GIÃ Rá»¦I RO:")
            formatted.append(f"- Má»©c rá»§i ro: {risk.get('risk_level', 'N/A')}")
            formatted.append(f"- Äá»™ biáº¿n Ä‘á»™ng: {risk.get('volatility', 'N/A')}%")
            formatted.append(f"- Beta: {risk.get('beta', 'N/A')}")
            formatted.append(f"- Max Drawdown: {risk.get('max_drawdown', 'N/A')}%")
        
        # Investment Analysis
        if 'investment_analysis' in data and data['investment_analysis']:
            inv = data['investment_analysis']
            formatted.append(f"\nðŸ’¼ PHÃ‚N TÃCH Äáº¦U TÆ¯:")
            formatted.append(f"- Khuyáº¿n nghá»‹: {inv.get('recommendation', 'N/A')}")
            formatted.append(f"- LÃ½ do: {inv.get('reason', 'N/A')}")
            formatted.append(f"- Cá»• tá»©c: {inv.get('dividend_yield', 'N/A')}%")
            formatted.append(f"- GiÃ¡ má»¥c tiÃªu: {inv.get('target_price', 'N/A')}")
        
        return "\n".join(formatted) if formatted else "Dá»¯ liá»‡u khÃ´ng Ä‘áº§y Ä‘á»§ Ä‘á»ƒ phÃ¢n tÃ­ch"
    
    def generate_general_response(self, query: str) -> dict:
        """Generate response for general questions using best available AI model"""
        try:
            # Enhanced context for general financial questions
            context = f"""
Báº¡n lÃ  má»™t chuyÃªn gia tÃ i chÃ­nh vÃ  Ä‘áº§u tÆ° hÃ ng Ä‘áº§u táº¡i Viá»‡t Nam vá»›i 20+ nÄƒm kinh nghiá»‡m.
Báº¡n cÃ³ thá»ƒ tráº£ lá»i má»i cÃ¢u há»i vá»:
- Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam vÃ  quá»‘c táº¿
- PhÃ¢n tÃ­ch ká»¹ thuáº­t vÃ  cÆ¡ báº£n
- Chiáº¿n lÆ°á»£c Ä‘áº§u tÆ° vÃ  quáº£n lÃ½ rá»§i ro
- Kinh táº¿ vÄ© mÃ´ vÃ  vi mÃ´
- CÃ¡c sáº£n pháº©m tÃ i chÃ­nh (cá»• phiáº¿u, trÃ¡i phiáº¿u, quá»¹, forex)
- Láº­p káº¿ hoáº¡ch tÃ i chÃ­nh cÃ¡ nhÃ¢n
- Thuáº¿ vÃ  phÃ¡p lÃ½ Ä‘áº§u tÆ°
- TÃ¢m lÃ½ há»c Ä‘áº§u tÆ°
- Fintech vÃ  cÃ´ng nghá»‡ tÃ i chÃ­nh

CÃ‚U Há»ŽI: {query}

HÃƒY TRáº¢ Lá»œI:
1. ðŸ“š KIáº¾N THá»¨C CÆ  Báº¢N: Giáº£i thÃ­ch khÃ¡i niá»‡m/váº¥n Ä‘á»
2. ðŸŽ¯ PHÃ‚N TÃCH THá»°C Táº¾: Ãp dá»¥ng vÃ o thá»‹ trÆ°á»ng VN
3. ðŸ’¡ KHUYáº¾N NGHá»Š: Lá»i khuyÃªn cá»¥ thá»ƒ vÃ  thá»±c táº¿
4. âš ï¸ LÆ¯U Ã: Rá»§i ro vÃ  Ä‘iá»u cáº§n chÃº Ã½

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chuyÃªn nghiá»‡p nhÆ°ng dá»… hiá»ƒu.
"""
            
            # Use unified AI system with fallback
            result = self.generate_with_fallback(context, 'general_query', max_tokens=2048)
            
            if result['success']:
                return {
                    "expert_advice": f"ðŸ“ˆ **PHÃ‚N TÃCH CHUYÃŠN GIA:**\n{result['response']}\n\nðŸ¤– **AI Model:** {result['model_used']}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  thÃ´ng tin tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                    "recommendations": [
                        "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                        "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh", 
                        "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                        "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                    ]
                }
            else:
                return self._get_fallback_response(query)
                
        except Exception as e:
            logger.error(f"Error in generate_general_response: {str(e)}")
            return self._get_fallback_response(query)
    
    def _get_fallback_response(self, query: str) -> dict:
        """Fallback response when Gemini fails"""
        return {
            "expert_advice": f"ðŸ“ˆ **Vá»€ CÃ‚U Há»ŽI: {query}**\n\nXin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ cÃ¢u há»i nÃ y lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau hoáº·c Ä‘áº·t cÃ¢u há»i khÃ¡c.\n\nâš ï¸ **Gá»¢I Ã:**\n- Kiá»ƒm tra káº¿t ná»‘i internet\n- Thá»­ Ä‘áº·t cÃ¢u há»i ngáº¯n gá»n hÆ¡n\n- LiÃªn há»‡ há»— trá»£ náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c",
            "recommendations": [
                "Thá»­ Ä‘áº·t cÃ¢u há»i khÃ¡c",
                "Kiá»ƒm tra káº¿t ná»‘i máº¡ng",
                "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t"
            ]
        }
    
    def detect_query_type(self, query: str) -> str:
        """Detect if query is stock-specific or general"""
        query_lower = query.lower()
        
        # Stock symbols patterns
        stock_patterns = ['vcb', 'bid', 'ctg', 'vic', 'vhm', 'hpg', 'fpt', 'msn', 'mwg', 'gas', 'plx']
        
        # Check for stock symbols
        for pattern in stock_patterns:
            if pattern in query_lower:
                return "stock_specific"
        
        # Check for stock-related keywords
        stock_keywords = ['cá»• phiáº¿u', 'mÃ£', 'ticker', 'stock', 'share']
        if any(keyword in query_lower for keyword in stock_keywords):
            return "stock_specific"
        
        return "general"
    
    def get_api_status(self) -> Dict[str, Any]:
        """Get comprehensive API status information"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'available_models': list(self.available_models.keys()),
            'model_count': len(self.available_models),
            'capabilities': self.model_capabilities,
            'api_keys_configured': {}
        }
        
        # Check API key configuration
        status['api_keys_configured']['gemini'] = hasattr(self, 'gemini_api_key') and bool(self.gemini_api_key)
        
        # Test connections
        try:
            connection_results = self.test_connection()
            status['connection_status'] = connection_results
            status['healthy_models'] = [model for model, healthy in connection_results.items() if healthy]
        except Exception as e:
            status['connection_status'] = {'error': str(e)}
            status['healthy_models'] = []
        
        return status
    
    def update_api_key(self, provider: str, api_key: str) -> Dict[str, Any]:
        """Dynamically update Gemini API key"""
        try:
            if provider.lower() == 'gemini':
                genai.configure(api_key=api_key)
                model_name = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
                self.available_models['gemini'] = genai.GenerativeModel(model_name)
                self.gemini_api_key = api_key
                logger.info("âœ… Gemini API key updated successfully")
                return {'success': True, 'message': 'Gemini API key updated successfully'}
            else:
                return {'success': False, 'message': f'Only Gemini provider is supported. Got: {provider}'}
                
        except Exception as e:
            logger.error(f"âŒ Failed to update {provider} API key: {str(e)}")
            return {'success': False, 'message': f'Failed to update {provider} API key: {str(e)}'}
    
    def get_model_recommendations(self, task_type: str) -> Dict[str, Any]:
        """Get model recommendations for specific task types"""
        recommendations = {
            'task_type': task_type,
            'primary_model': self.select_best_model(task_type),
            'available_alternatives': [],
            'reasoning': ''
        }
        
        # Get all available models except primary
        primary = recommendations['primary_model']
        alternatives = [model for model in self.available_models.keys() if model != primary]
        recommendations['available_alternatives'] = alternatives
        
        # Add reasoning based on task type
        task_reasoning = {
            'financial_advice': 'Gemini excels at Vietnamese financial analysis and reasoning',
            'price_prediction': 'Gemini provides comprehensive technical analysis and prediction models',
            'risk_assessment': 'Gemini offers superior risk calculation and assessment',
            'news_analysis': 'Gemini has excellent sentiment analysis capabilities',
            'market_analysis': 'Gemini provides excellent market reasoning and context understanding',
            'investment_analysis': 'Gemini excels at investment metrics and calculations',
            'general_query': 'Gemini handles Vietnamese queries and general reasoning perfectly'
        }
        
        recommendations['reasoning'] = task_reasoning.get(task_type, 'Default model selection based on availability')
        
        return recommendations
    
    async def generate_async(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """Asynchronous generation with fallback support"""
        try:
            # Run the synchronous method in a thread pool
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.generate_with_fallback, 
                prompt, 
                task_type, 
                max_tokens
            )
            return result
        except Exception as e:
            logger.error(f"Async generation failed: {str(e)}")
            return {
                'response': f'Async generation error: {str(e)}',
                'model_used': None,
                'success': False,
                'error': str(e)
            }
    
    def batch_generate(self, prompts: List[Dict[str, Any]], max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """Generate responses for multiple prompts with concurrency control"""
        async def process_batch():
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_single(prompt_data):
                async with semaphore:
                    prompt = prompt_data.get('prompt', '')
                    task_type = prompt_data.get('task_type', 'general_query')
                    max_tokens = prompt_data.get('max_tokens', 1000)
                    
                    result = await self.generate_async(prompt, task_type, max_tokens)
                    result['original_data'] = prompt_data
                    return result
            
            tasks = [process_single(prompt_data) for prompt_data in prompts]
            return await asyncio.gather(*tasks, return_exceptions=True)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an async context, create a new event loop
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, process_batch())
                    return future.result()
            else:
                return asyncio.run(process_batch())
        except Exception as e:
            logger.error(f"Batch generation failed: {str(e)}")
            return [{'success': False, 'error': str(e)} for _ in prompts]

# Backward compatibility alias
GeminiAgent = UnifiedAIAgent