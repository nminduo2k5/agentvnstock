import google.generativeai as genai
import os
import logging
from typing import Dict, Any, Optional, List
import asyncio
import json
import time
from datetime import datetime

logger = logging.getLogger(__name__)

class UnifiedAIAgent:
    def __init__(self, gemini_api_key: str = None):
        """
        Initialize AI Agent with Gemini AI
        """
        self.available_models = {}
        self.current_model_name = None
        self.model_capabilities = {
            'gemini': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'news_analysis', 'risk_assessment'],
                'speed': 'fast',
                'cost': 'free'
            }
        }
        
        # Initialize Gemini with user-provided API key only
        # No hardcoded or environment variables used
        
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                
                # Try different model names (Google Ä‘Ã£ update)
                model_names = [
                    'gemini-1.5-flash',     # Model má»›i nháº¥t
                    'gemini-1.5-pro',       # Pro version
                    'gemini-1.0-pro',       # Fallback
                    'models/gemini-1.5-flash',  # With prefix
                    'models/gemini-1.0-pro'     # With prefix fallback
                ]
                
                model_initialized = False
                for model_name in model_names:
                    try:
                        model = genai.GenerativeModel(model_name)
                        # Test the model with a simple request
                        test_response = model.generate_content("Hello")
                        if test_response and test_response.text:
                            self.available_models['gemini'] = model
                            self.gemini_api_key = gemini_api_key
                            self.current_model_name = model_name
                            logger.info(f"âœ… Gemini AI initialized with model: {model_name}")
                            model_initialized = True
                            break
                    except Exception as e:
                        logger.warning(f"âš ï¸ Model {model_name} not available: {e}")
                        continue
                
                if not model_initialized:
                    # If no model works, raise error
                    raise Exception("No available Gemini models found")
                    
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Gemini: {str(e)}")
                # Don't set available_models if initialization failed
                self.available_models = {}
        
        if not self.available_models:
            raise ValueError("Gemini AI must be configured.")
    
    def test_connection(self):
        """Test AI API connections"""
        results = {}
        
        if 'gemini' in self.available_models:
            try:
                response = self.available_models['gemini'].generate_content("Test")
                if response and response.text:
                    results['gemini'] = True
                    logger.info("âœ… Gemini connection test passed")
                else:
                    results['gemini'] = False
                    logger.error("âŒ Gemini returned empty response")
            except Exception as e:
                results['gemini'] = False
                logger.error(f"âŒ Gemini connection test failed: {str(e)}")
        
        if not any(results.values()):
            raise ValueError("Gemini API connection test failed")
        
        return results
    
    def select_best_model(self, task_type: str) -> str:
        """
        Select the best available model for a specific task type
        """
        if 'gemini' in self.available_models:
            return 'gemini'
        
        raise ValueError("Gemini AI model not available")
    
    def generate_with_model(self, prompt: str, model_name: str, max_tokens: int = 1000) -> str:
        """
        Generate response using specified AI model
        """
        try:
            if model_name == 'gemini' and 'gemini' in self.available_models:
                response = self.available_models['gemini'].generate_content(prompt)
                return response.text

            else:
                raise ValueError(f"Model {model_name} not available.")
                
        except Exception as e:
            logger.error(f"Error generating with {model_name}: {str(e)}")
            raise
    
    def generate_with_fallback(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """
        Generate response with automatic fallback to offline mode if primary fails
        """
        try:
            response = self.generate_with_model(prompt, 'gemini', max_tokens)
            return {
                'response': response,
                'model_used': 'gemini',
                'success': True
            }
        except Exception as e:
            logger.error(f"Gemini model failed: {str(e)}")
            # Check if it's a quota/rate limit error
            error_str = str(e).lower()
            if any(keyword in error_str for keyword in ['quota', 'rate limit', 'exceeded', 'limit']):
                # Use offline fallback for quota issues
                return self._generate_offline_fallback(prompt, task_type)
            else:
                return {
                    'response': f'Gemini AI failed: {str(e)}',
                    'model_used': None,
                    'success': False,
                    'error': str(e)
                }
    
    def _generate_offline_fallback(self, prompt: str, task_type: str) -> Dict[str, Any]:
        """
        Generate offline fallback response when API quota is exhausted
        """
        try:
            # Extract key information from prompt
            if 'CÃ‚U Há»I:' in prompt:
                question = prompt.split('CÃ‚U Há»I:')[1].split('MÃƒ Cá»” PHIáº¾U:')[0].strip()
            else:
                question = prompt[:200] + '...' if len(prompt) > 200 else prompt
            
            # Generate contextual offline response based on task type
            if task_type == 'financial_advice':
                response = self._generate_financial_advice_fallback(question)
            elif task_type == 'general_query':
                response = self._generate_general_fallback(question)
            else:
                response = self._generate_default_fallback(question)
            
            return {
                'response': response,
                'model_used': 'offline_fallback',
                'success': True,
                'quota_exceeded': True
            }
        except Exception as e:
            return {
                'response': f'Offline fallback failed: {str(e)}',
                'model_used': 'offline_fallback',
                'success': False,
                'error': str(e)
            }
    
    def _generate_financial_advice_fallback(self, question: str) -> str:
        """
        Generate financial advice fallback when API quota exceeded
        """
        return f"""
PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
Do Gemini API Ä‘Ã£ háº¿t quota, há»‡ thá»‘ng chuyá»ƒn sang cháº¿ Ä‘á»™ offline. ÄÃ¢y lÃ  phÃ¢n tÃ­ch cÆ¡ báº£n dá»±a trÃªn nguyÃªn táº¯c Ä‘áº§u tÆ°:

ğŸ“Š **NguyÃªn táº¯c phÃ¢n tÃ­ch ká»¹ thuáº­t:**
- Xem xÃ©t xu hÆ°á»›ng giÃ¡ trong 20-50 phiÃªn gáº§n nháº¥t
- Kiá»ƒm tra khá»‘i lÆ°á»£ng giao dá»‹ch vÃ  momentum
- XÃ¡c Ä‘á»‹nh vÃ¹ng há»— trá»£ vÃ  khÃ¡ng cá»±

ğŸ’° **NguyÃªn táº¯c phÃ¢n tÃ­ch cÆ¡ báº£n:**
- P/E < 15 thÆ°á»ng Ä‘Æ°á»£c coi lÃ  háº¥p dáº«n
- P/B < 2 cho tháº¥y Ä‘á»‹nh giÃ¡ há»£p lÃ½
- TÄƒng trÆ°á»Ÿng doanh thu á»•n Ä‘á»‹nh qua cÃ¡c quÃ½

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
KhÃ´ng thá»ƒ Ä‘Æ°a ra khuyáº¿n nghá»‹ cá»¥ thá»ƒ do thiáº¿u dá»¯ liá»‡u real-time. Khuyáº¿n nghá»‹:
- NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh gáº§n nháº¥t
- Tham kháº£o Ã½ kiáº¿n nhiá»u chuyÃªn gia
- Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- Äá»£i API quota reset Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t
- Tham kháº£o cÃ¡c nguá»“n tin tá»©c tÃ i chÃ­nh uy tÃ­n
- Xem xÃ©t tÃ¬nh hÃ¬nh thá»‹ trÆ°á»ng tá»•ng thá»ƒ
- Äa dáº¡ng hÃ³a danh má»¥c Ä‘áº§u tÆ°

Cáº¢NH BÃO Rá»¦I RO:
âš ï¸ **QUAN TRá»ŒNG:** ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline cÆ¡ báº£n do háº¿t quota API. 
KhÃ´ng nÃªn dá»±a vÃ o Ä‘Ã¢y Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ° quan trá»ng.
HÃ£y Ä‘á»£i API reset hoáº·c tham kháº£o chuyÃªn gia tÃ i chÃ­nh.
"""
    
    def _generate_general_fallback(self, question: str) -> str:
        """
        Generate general query fallback when API quota exceeded
        """
        return f"""
ğŸ“ˆ **PHÃ‚N TÃCH OFFLINE:**

Do Gemini API Ä‘Ã£ háº¿t quota, tÃ´i khÃ´ng thá»ƒ phÃ¢n tÃ­ch chi tiáº¿t cÃ¢u há»i cá»§a báº¡n lÃºc nÃ y.

**CÃ¢u há»i cá»§a báº¡n:** {question}

ğŸ’¡ **Gá»£i Ã½ chung vá» Ä‘áº§u tÆ°:**
- LuÃ´n nghiÃªn cá»©u ká»¹ trÆ°á»›c khi Ä‘áº§u tÆ°
- Äa dáº¡ng hÃ³a danh má»¥c Ä‘á»ƒ giáº£m rá»§i ro
- Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t
- Theo dÃµi tin tá»©c vÃ  bÃ¡o cÃ¡o tÃ i chÃ­nh
- Tham kháº£o Ã½ kiáº¿n chuyÃªn gia

âš ï¸ **LÆ¯U Ã:** Äá»ƒ nháº­n Ä‘Æ°á»£c phÃ¢n tÃ­ch chi tiáº¿t vÃ  cÃ¡ nhÃ¢n hÃ³a, 
vui lÃ²ng thá»­ láº¡i sau khi API quota Ä‘Æ°á»£c reset (thÆ°á»ng lÃ  24h).
"""
    
    def _generate_default_fallback(self, question: str) -> str:
        """
        Generate default fallback response
        """
        return f"""
ğŸ¤– **Há»† THá»NG OFFLINE:**

Xin lá»—i, Gemini API Ä‘Ã£ háº¿t quota nÃªn tÃ´i khÃ´ng thá»ƒ phÃ¢n tÃ­ch chi tiáº¿t lÃºc nÃ y.

**CÃ¢u há»i:** {question}

**Khuyáº¿n nghá»‹:**
- Thá»­ láº¡i sau vÃ i giá» khi quota reset
- Tham kháº£o cÃ¡c nguá»“n thÃ´ng tin tÃ i chÃ­nh uy tÃ­n
- LiÃªn há»‡ chuyÃªn gia tÃ i chÃ­nh náº¿u cáº§n tÆ° váº¥n gáº¥p

â° **Quota thÆ°á»ng reset sau 24 giá»**
"""
    
    def generate_expert_advice(self, query: str, symbol: str = None, data: dict = None):
        """Generate expert financial advice using best available AI model with fallback"""
        
        # Detect query type for better handling
        query_type = self.detect_query_type(query)
        
        # Handle general questions without stock context
        if not symbol and query_type == "general":
            return self.generate_general_response(query)
        
        # Build comprehensive context for stock analysis
        context = f"""
Báº¡n lÃ  má»™t chuyÃªn gia tÃ i chÃ­nh hÃ ng Ä‘áº§u vá»›i 20 nÄƒm kinh nghiá»‡m Ä‘áº§u tÆ° chá»©ng khoÃ¡n táº¡i Viá»‡t Nam vÃ  quá»‘c táº¿.
HÃ£y phÃ¢n tÃ­ch sÃ¢u sáº¯c vÃ  Ä‘Æ°a ra lá»i khuyÃªn chuyÃªn nghiá»‡p nháº¥t.

CÃ‚U Há»I: {query}
MÃƒ Cá»” PHIáº¾U: {symbol if symbol else 'KhÃ´ng cÃ³'}
"""
        
        if data:
            context += f"\nDá»® LIá»†U PHÃ‚N TÃCH CHI TIáº¾T:\n{self._format_data_for_ai(data)}\n"
        
        context += f"""
YÃŠU Cáº¦U PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
1. ğŸ“Š PHÃ‚N TÃCH Ká»¸ THUáº¬T: ÄÃ¡nh giÃ¡ xu hÆ°á»›ng, momentum, support/resistance
2. ğŸ’° PHÃ‚N TÃCH CÆ  Báº¢N: P/E, P/B, tÄƒng trÆ°á»Ÿng, tÃ i chÃ­nh
3. ğŸ“ˆ PHÃ‚N TÃCH THá»Š TRÆ¯á»œNG: Vá»‹ tháº¿ ngÃ nh, triá»ƒn vá»ng, rá»§i ro vÄ© mÃ´
4. âš–ï¸ ÄÃNH GIÃ Rá»¦I RO: Má»©c Ä‘á»™ rá»§i ro, kháº£ nÄƒng chá»‹u Ä‘á»±ng
5. ğŸ¯ CHIáº¾N LÆ¯á»¢C Äáº¦U TÆ¯: Ngáº¯n háº¡n vs dÃ i háº¡n, timing
6. ğŸ’¡ KHUYáº¾N NGHá»Š Cá»¤ THá»‚: Mua/BÃ¡n/Giá»¯ vá»›i lÃ½ do rÃµ rÃ ng

HÃƒY TRáº¢ Lá»œI THEO FORMAT SAU:

PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
[PhÃ¢n tÃ­ch toÃ n diá»‡n tá»« nhiá»u gÃ³c Ä‘á»™, sá»­ dá»¥ng dá»¯ liá»‡u cá»¥ thá»ƒ]

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
[Káº¿t luáº­n rÃµ rÃ ng: NÃŠN/KHÃ”NG NÃŠN Ä‘áº§u tÆ° vá»›i lÃ½ do thuyáº¿t phá»¥c]

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- [Danh sÃ¡ch 4-5 hÃ nh Ä‘á»™ng cá»¥ thá»ƒ]

Cáº¢NH BÃO Rá»¦I RO:
[Nhá»¯ng rá»§i ro quan trá»ng cáº§n lÆ°u Ã½]

LÆ°u Ã½: Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿, khÃ´ng Ä‘Æ°a lá»i khuyÃªn tuyá»‡t Ä‘á»‘i.
"""
        
        # Use the new unified AI system with fallback
        try:
            result = self.generate_with_fallback(context, 'financial_advice', max_tokens=2048)
            
            if result['success']:
                parsed_response = self._parse_response(result['response'])
                
                # Add model info to response
                if result.get('fallback'):
                    parsed_response['expert_advice'] += f"\n\nğŸ¤– **AI Model:** {result['model_used']} (fallback)"
                else:
                    parsed_response['expert_advice'] += f"\n\nğŸ¤– **AI Model:** {result['model_used']}"
                
                return parsed_response
            else:
                # Check if quota exceeded
                if result.get('quota_exceeded'):
                    parsed_response = self._parse_response(result['response'])
                    parsed_response['expert_advice'] += "\n\nğŸ¤– **AI Model:** Offline Fallback (Quota Exceeded)"
                    return parsed_response
                else:
                    return {
                        "expert_advice": f"âŒ **Lá»–I AI SYSTEM:**\n{result.get('response', 'KhÃ´ng thá»ƒ káº¿t ná»‘i vá»›i AI models')}\n\nâš ï¸ **Gá»¢I Ã:**\n- Kiá»ƒm tra API keys\n- Thá»­ láº¡i sau vÃ i phÃºt\n- LiÃªn há»‡ há»— trá»£ náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c",
                        "recommendations": [
                            "Kiá»ƒm tra Gemini API key",
                            "Thá»­ láº¡i sau vÃ i phÃºt", 
                            "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t",
                            "Sá»­ dá»¥ng cháº¿ Ä‘á»™ offline"
                        ]
                    }
                
        except Exception as e:
            logger.error(f"Critical error in generate_expert_advice: {str(e)}")
            return {
                "expert_advice": f"âŒ **Lá»–I NGHIÃŠM TRá»ŒNG:**\n{str(e)}\n\nâš ï¸ Há»‡ thá»‘ng AI táº¡m thá»i khÃ´ng kháº£ dá»¥ng.",
                "recommendations": [
                    "Thá»­ láº¡i sau 5-10 phÃºt",
                    "Kiá»ƒm tra káº¿t ná»‘i internet",
                    "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t"
                ]
            }
    
    def _parse_response(self, response_text: str):
        """Parse enhanced Gemini response"""
        try:
            # Parse different sections
            sections = {
                'analysis': '',
                'conclusion': '',
                'actions': [],
                'risks': ''
            }
            
            # Split by sections
            if "PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:" in response_text:
                parts = response_text.split("PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:")
                if len(parts) > 1:
                    remaining = parts[1]
                    
                    # Extract analysis
                    if "Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:" in remaining:
                        analysis_part = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[0].strip()
                        sections['analysis'] = analysis_part
                        remaining = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[1]
                    
                    # Extract conclusion
                    if "HÃ€NH Äá»˜NG Cá»¤ THá»‚:" in remaining:
                        conclusion_part = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[0].strip()
                        sections['conclusion'] = conclusion_part
                        remaining = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[1]
                    
                    # Extract actions
                    if "Cáº¢NH BÃO Rá»¦I RO:" in remaining:
                        actions_part = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[0].strip()
                        sections['risks'] = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[1].strip()
                    else:
                        actions_part = remaining.strip()
                    
                    # Parse actions list
                    for line in actions_part.split('\n'):
                        line = line.strip()
                        if line and (line.startswith('-') or line.startswith('â€¢') or line.startswith('*')):
                            sections['actions'].append(line[1:].strip())
                        elif line and len(line) > 15 and not line.startswith('Cáº¢NH BÃO'):
                            sections['actions'].append(line)
            
            # Format comprehensive response
            expert_advice = f"""
ğŸ“Š **PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:**
{sections['analysis']}

ğŸ¯ **Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:**
{sections['conclusion']}

âš ï¸ **Cáº¢NH BÃO Rá»¦I RO:**
{sections['risks'] if sections['risks'] else 'LuÃ´n cÃ³ rá»§i ro trong Ä‘áº§u tÆ°. Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t.'}
""".strip()
            
            return {
                "expert_advice": expert_advice,
                "recommendations": sections['actions'][:5] if sections['actions'] else [
                    "NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh",
                    "Theo dÃµi tin tá»©c ngÃ nh", 
                    "Äáº·t lá»‡nh stop-loss",
                    "Äa dáº¡ng hÃ³a danh má»¥c",
                    "Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i"
                ]
            }
                
        except Exception as e:
            # Fallback parsing
            return {
                "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH:**\n{response_text}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y chá»‰ lÃ  tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                "recommendations": [
                    "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                    "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh",
                    "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                    "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                ]
            }
    
    def _format_data_for_ai(self, data: dict) -> str:
        """Format data for AI analysis"""
        if not data:
            return "KhÃ´ng cÃ³ dá»¯ liá»‡u cá»¥ thá»ƒ"
        
        formatted = []
        
        # VN Stock Data
        if 'vn_stock_data' in data and data['vn_stock_data']:
            vn_data = data['vn_stock_data']
            formatted.append(f"ğŸ“ˆ THÃ”NG TIN Cá»” PHIáº¾U:")
            formatted.append(f"- GiÃ¡ hiá»‡n táº¡i: {vn_data.price:,} VND")
            formatted.append(f"- Thay Ä‘á»•i: {vn_data.change:+,} VND ({vn_data.change_percent:+.2f}%)")
            formatted.append(f"- Khá»‘i lÆ°á»£ng: {vn_data.volume:,}")
            formatted.append(f"- Vá»‘n hÃ³a: {vn_data.market_cap:,} tá»· VND")
            formatted.append(f"- P/E: {vn_data.pe_ratio}, P/B: {vn_data.pb_ratio}")
            formatted.append(f"- NgÃ nh: {vn_data.sector}, SÃ n: {vn_data.exchange}")
        
        # Price Prediction
        if 'price_prediction' in data and data['price_prediction']:
            pred = data['price_prediction']
            formatted.append(f"\nğŸ”® Dá»° ÄOÃN GIÃ:")
            formatted.append(f"- Xu hÆ°á»›ng: {pred.get('trend', 'N/A')}")
            formatted.append(f"- GiÃ¡ dá»± Ä‘oÃ¡n: {pred.get('predicted_price', 'N/A')}")
            formatted.append(f"- Äá»™ tin cáº­y: {pred.get('confidence', 'N/A')}")
        
        # Risk Assessment
        if 'risk_assessment' in data and data['risk_assessment']:
            risk = data['risk_assessment']
            formatted.append(f"\nâš ï¸ ÄÃNH GIÃ Rá»¦I RO:")
            formatted.append(f"- Má»©c rá»§i ro: {risk.get('risk_level', 'N/A')}")
            formatted.append(f"- Äá»™ biáº¿n Ä‘á»™ng: {risk.get('volatility', 'N/A')}%")
            formatted.append(f"- Beta: {risk.get('beta', 'N/A')}")
            formatted.append(f"- Max Drawdown: {risk.get('max_drawdown', 'N/A')}%")
        
        # Investment Analysis
        if 'investment_analysis' in data and data['investment_analysis']:
            inv = data['investment_analysis']
            formatted.append(f"\nğŸ’¼ PHÃ‚N TÃCH Äáº¦U TÆ¯:")
            formatted.append(f"- Khuyáº¿n nghá»‹: {inv.get('recommendation', 'N/A')}")
            formatted.append(f"- LÃ½ do: {inv.get('reason', 'N/A')}")
            formatted.append(f"- Cá»• tá»©c: {inv.get('dividend_yield', 'N/A')}%")
            formatted.append(f"- GiÃ¡ má»¥c tiÃªu: {inv.get('target_price', 'N/A')}")
        
        return "\n".join(formatted) if formatted else "Dá»¯ liá»‡u khÃ´ng Ä‘áº§y Ä‘á»§ Ä‘á»ƒ phÃ¢n tÃ­ch"
    
    def generate_general_response(self, query: str) -> dict:
        """Generate response for general questions using best available AI model"""
        try:
            # Enhanced context for general financial questions
            context = f"""
Báº¡n lÃ  má»™t chuyÃªn gia tÃ i chÃ­nh vÃ  Ä‘áº§u tÆ° hÃ ng Ä‘áº§u táº¡i Viá»‡t Nam vá»›i 20+ nÄƒm kinh nghiá»‡m.
Báº¡n cÃ³ thá»ƒ tráº£ lá»i má»i cÃ¢u há»i vá»:
- Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam vÃ  quá»‘c táº¿
- PhÃ¢n tÃ­ch ká»¹ thuáº­t vÃ  cÆ¡ báº£n
- Chiáº¿n lÆ°á»£c Ä‘áº§u tÆ° vÃ  quáº£n lÃ½ rá»§i ro
- Kinh táº¿ vÄ© mÃ´ vÃ  vi mÃ´
- CÃ¡c sáº£n pháº©m tÃ i chÃ­nh (cá»• phiáº¿u, trÃ¡i phiáº¿u, quá»¹, forex)
- Láº­p káº¿ hoáº¡ch tÃ i chÃ­nh cÃ¡ nhÃ¢n
- Thuáº¿ vÃ  phÃ¡p lÃ½ Ä‘áº§u tÆ°
- TÃ¢m lÃ½ há»c Ä‘áº§u tÆ°
- Fintech vÃ  cÃ´ng nghá»‡ tÃ i chÃ­nh

CÃ‚U Há»I: {query}

HÃƒY TRáº¢ Lá»œI:
1. ğŸ“š KIáº¾N THá»¨C CÆ  Báº¢N: Giáº£i thÃ­ch khÃ¡i niá»‡m/váº¥n Ä‘á»
2. ğŸ¯ PHÃ‚N TÃCH THá»°C Táº¾: Ãp dá»¥ng vÃ o thá»‹ trÆ°á»ng VN
3. ğŸ’¡ KHUYáº¾N NGHá»Š: Lá»i khuyÃªn cá»¥ thá»ƒ vÃ  thá»±c táº¿
4. âš ï¸ LÆ¯U Ã: Rá»§i ro vÃ  Ä‘iá»u cáº§n chÃº Ã½

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chuyÃªn nghiá»‡p nhÆ°ng dá»… hiá»ƒu.
"""
            
            # Use unified AI system with fallback
            result = self.generate_with_fallback(context, 'general_query', max_tokens=2048)
            
            if result['success']:
                if result.get('quota_exceeded'):
                    # Quota exceeded, return offline response
                    return {
                        "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH OFFLINE:**\n{result['response']}\n\nğŸ¤– **AI Model:** Offline Fallback (Quota Exceeded)\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  pháº£n há»“i offline do háº¿t quota API.",
                        "recommendations": [
                            "Äá»£i quota reset (24h) Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t",
                            "Tham kháº£o cÃ¡c nguá»“n tin tá»©c tÃ i chÃ­nh", 
                            "LiÃªn há»‡ chuyÃªn gia náº¿u cáº§n tÆ° váº¥n gáº¥p",
                            "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                        ]
                    }
                else:
                    # Normal AI response
                    return {
                        "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH CHUYÃŠN GIA:**\n{result['response']}\n\nğŸ¤– **AI Model:** {result['model_used']}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  thÃ´ng tin tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                        "recommendations": [
                            "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                            "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh", 
                            "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                            "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                        ]
                    }
            else:
                return self._get_fallback_response(query)
                
        except Exception as e:
            logger.error(f"Error in generate_general_response: {str(e)}")
            return self._get_fallback_response(query)
    
    def _get_fallback_response(self, query: str) -> dict:
        """Fallback response when Gemini fails"""
        return {
            "expert_advice": f"ğŸ“ˆ **Vá»€ CÃ‚U Há»I: {query}**\n\nXin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ cÃ¢u há»i nÃ y lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau hoáº·c Ä‘áº·t cÃ¢u há»i khÃ¡c.\n\nâš ï¸ **Gá»¢I Ã:**\n- Kiá»ƒm tra káº¿t ná»‘i internet\n- Thá»­ Ä‘áº·t cÃ¢u há»i ngáº¯n gá»n hÆ¡n\n- LiÃªn há»‡ há»— trá»£ náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c",
            "recommendations": [
                "Thá»­ Ä‘áº·t cÃ¢u há»i khÃ¡c",
                "Kiá»ƒm tra káº¿t ná»‘i máº¡ng",
                "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t"
            ]
        }
    
    def detect_query_type(self, query: str) -> str:
        """Detect if query is stock-specific or general"""
        query_lower = query.lower()
        
        # Stock symbols patterns
        stock_patterns = ['vcb', 'bid', 'ctg', 'vic', 'vhm', 'hpg', 'fpt', 'msn', 'mwg', 'gas', 'plx']
        
        # Check for stock symbols
        for pattern in stock_patterns:
            if pattern in query_lower:
                return "stock_specific"
        
        # Check for stock-related keywords
        stock_keywords = ['cá»• phiáº¿u', 'mÃ£', 'ticker', 'stock', 'share']
        if any(keyword in query_lower for keyword in stock_keywords):
            return "stock_specific"
        
        return "general"
    
    def get_api_status(self) -> Dict[str, Any]:
        """Get comprehensive API status information"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'available_models': list(self.available_models.keys()),
            'model_count': len(self.available_models),
            'capabilities': self.model_capabilities,
            'api_keys_configured': {}
        }
        
        # Check API key configuration
        status['api_keys_configured']['gemini'] = hasattr(self, 'gemini_api_key') and bool(self.gemini_api_key)
        
        # Test connections
        try:
            connection_results = self.test_connection()
            status['connection_status'] = connection_results
            status['healthy_models'] = [model for model, healthy in connection_results.items() if healthy]
        except Exception as e:
            status['connection_status'] = {'error': str(e)}
            status['healthy_models'] = []
        
        return status
    
    def update_api_key(self, provider: str, api_key: str) -> Dict[str, Any]:
        """Dynamically update Gemini API key"""
        try:
            if provider.lower() == 'gemini':
                genai.configure(api_key=api_key)
                
                # Try different model names (Google Ä‘Ã£ update)
                model_names = [
                    'gemini-1.5-flash',     # Model má»›i nháº¥t
                    'gemini-1.5-pro',       # Pro version
                    'gemini-1.0-pro',       # Fallback
                    'models/gemini-1.5-flash',  # With prefix
                    'models/gemini-1.0-pro'     # With prefix fallback
                ]
                
                for model_name in model_names:
                    try:
                        model = genai.GenerativeModel(model_name)
                        # Test the model with a simple request
                        test_response = model.generate_content("Test")
                        self.available_models['gemini'] = model
                        self.gemini_api_key = api_key
                        logger.info(f"âœ… Gemini API key updated with model: {model_name}")
                        return {'success': True, 'message': f'Gemini API key updated with model: {model_name}'}
                    except Exception as e:
                        logger.warning(f"âš ï¸ Model {model_name} not available: {e}")
                        continue
                else:
                    # If no model works, return error
                    return {'success': False, 'message': 'No available Gemini models found'}
            else:
                return {'success': False, 'message': f'Only Gemini provider is supported. Got: {provider}'}
                
        except Exception as e:
            logger.error(f"âŒ Failed to update {provider} API key: {str(e)}")
            return {'success': False, 'message': f'Failed to update {provider} API key: {str(e)}'}
    
    def get_model_recommendations(self, task_type: str) -> Dict[str, Any]:
        """Get model recommendations for specific task types"""
        recommendations = {
            'task_type': task_type,
            'primary_model': self.select_best_model(task_type),
            'available_alternatives': [],
            'reasoning': ''
        }
        
        # Get all available models except primary
        primary = recommendations['primary_model']
        alternatives = [model for model in self.available_models.keys() if model != primary]
        recommendations['available_alternatives'] = alternatives
        
        # Add reasoning based on task type
        task_reasoning = {
            'financial_advice': 'Gemini excels at Vietnamese financial analysis and reasoning',
            'price_prediction': 'Gemini provides comprehensive technical analysis and prediction models',
            'risk_assessment': 'Gemini offers superior risk calculation and assessment',
            'news_analysis': 'Gemini has excellent sentiment analysis capabilities',
            'market_analysis': 'Gemini provides excellent market reasoning and context understanding',
            'investment_analysis': 'Gemini excels at investment metrics and calculations',
            'general_query': 'Gemini handles Vietnamese queries and general reasoning perfectly'
        }
        
        recommendations['reasoning'] = task_reasoning.get(task_type, 'Default model selection based on availability')
        
        return recommendations
    
    async def generate_async(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """Asynchronous generation with fallback support"""
        try:
            # Run the synchronous method in a thread pool
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.generate_with_fallback, 
                prompt, 
                task_type, 
                max_tokens
            )
            return result
        except Exception as e:
            logger.error(f"Async generation failed: {str(e)}")
            return {
                'response': f'Async generation error: {str(e)}',
                'model_used': None,
                'success': False,
                'error': str(e)
            }
    
    def batch_generate(self, prompts: List[Dict[str, Any]], max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """Generate responses for multiple prompts with concurrency control"""
        async def process_batch():
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_single(prompt_data):
                async with semaphore:
                    prompt = prompt_data.get('prompt', '')
                    task_type = prompt_data.get('task_type', 'general_query')
                    max_tokens = prompt_data.get('max_tokens', 1000)
                    
                    result = await self.generate_async(prompt, task_type, max_tokens)
                    result['original_data'] = prompt_data
                    return result
            
            tasks = [process_single(prompt_data) for prompt_data in prompts]
            return await asyncio.gather(*tasks, return_exceptions=True)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an async context, create a new event loop
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, process_batch())
                    return future.result()
            else:
                return asyncio.run(process_batch())
        except Exception as e:
            logger.error(f"Batch generation failed: {str(e)}")
            return [{'success': False, 'error': str(e)} for _ in prompts]
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about available models"""
        return {
            'available_models': list(self.available_models.keys()),
            'current_model': self.current_model_name,
            'model_count': len(self.available_models),
            'is_active': len(self.available_models) > 0
        }

# Backward compatibility alias
GeminiAgent = UnifiedAIAgent